/* HMT project:  Mandatory Ongoing Maintenance. 
*/


import com.github.rjeschke.txtmark.*
import org.apache.tools.ant.filters.*

buildscript {
  apply from: "versions.gradle"
    repositories {
        mavenCentral()
	maven {
	  url "http://beta.hpcc.uh.edu/nexus/content/groups/public"
	}
    }
    dependencies {
      classpath group: 'com.github.rjeschke', name: 'txtmark', version: '0.11'
      classpath group: 'org.homermultitext', name : 'citemgr' , version: citemgrVersion
      classpath group: 'org.homermultitext', name : 'hmt-utils' , version: hmtutilsVersion

      classpath group: 'edu.harvard.chs', name : 'cite' , version: citeVersion
      classpath group: 'edu.holycross.shot', name: 'hocuspocus', version : hocuspocusVersion

      classpath group: 'edu.harvard.chs', name : 'greekutils' , version: greekutilsVersion

      classpath group : 'edu.unc.epidoc', name: 'transcoder', version : transcoderVersion
      classpath group: 'net.sf.opencsv', name: 'opencsv', version: opencsvVersion


    }
}

//import org.homermultitext.utils.HmtTokenizer
import org.homermultitext.utils.HmtEditorialTokenization

import edu.harvard.chs.cite.CiteUrn
import edu.harvard.chs.cite.CtsUrn

import edu.holycross.shot.hocuspocus.Corpus

import org.homermultitext.citemanager.DseManager

import edu.harvard.chs.f1k.GreekNode

import edu.unc.epidoc.transcoder.TransCoder

import au.com.bytecode.opencsv.CSVReader
import groovy.xml.StreamingMarkupBuilder

import org.apache.tools.ant.filters.*

apply plugin: "base"
apply plugin:  "groovy"
apply plugin:  "maven"


apply from: "versions.gradle"


if (hasProperty('conf')) {
    System.err.println "Using configuration data from ${conf}"
    File confFile = new File(conf)
    if (! confFile.exists()) {
        throw new Exception("No configuration file ${conf} found.")
    }
    apply from: conf

} else {
    File confFile = new File("conf.gradle")
    if (! confFile.exists()) {
        throw new Exception("No configuration file ${conf} found.")
    }
    System.err.println "Using conf.gradle for configuration"
    apply from: "conf.gradle"
    
}



if (hasProperty('repoconf')) {
    System.err.println "Using repository locations data from ${repoconf}"
    File confFile = new File(repoconf)
    if (! confFile.exists()) {
        throw new Exception("No file ${repoconf} found.")
    }
    apply from: repoconf

} else {
    File confFile = new File("repos.gradle")
    if (! confFile.exists()) {
        throw new Exception("No configuration file ${confFile} found.")
    }
    System.err.println "Using repos.gradle for finding repositories."
    apply from: "repos.gradle"
}

if (hasProperty('baserepo')) {
  println "USE " + baserepo + " for repository base for DSE validation."
} else {
  println "NO base repository configured"
}

group = "org.homermultitext"
version = '1.0.test1'

repositories {
    mavenCentral()
    maven {
        url "http://beta.hpcc.uh.edu/nexus/content/groups/public"
    }
}

dependencies {
  compile group: 'org.codehaus.groovy', name: 'groovy-all', version: groovyVersion
  compile group: 'org.homermultitext', name : 'citemgr' , version: citemgrVersion
  compile group: 'edu.harvard.chs', name : 'cite' , version: citeVersion

  testCompile 'junit:junit:4.11'
  testCompile 'org.concordion:concordion:1.4.4'
  
  //testCompile group: 'junit', name: 'junit', version: junitVersion

}



task seeSettings() {
}
seeSettings.doLast () {
  println "persnames ${persnames}"
    println "placenames ${placenames}"
}

task palView() {
  description = "Creates HTML page displaying paleographic inventory data."
}

palView.doLast {
  if (! buildDir.exists()) {
    buildDir.mkdir()
  }
  File outputDir = new File(buildDir, "paleography")
  if (! outputDir.exists()) {
    outputDir.mkdir()
  }
  File outputFile = new File(outputDir, "viewer.html")


  String urnBase = "http://beta.hpcc.uh.edu/tomcat/hmt-digital/images?request=GetBinaryImage&w=9000&urn="

  File palFile = new File(paleo)
  CSVReader reader = new CSVReader(new FileReader(palFile))

  StreamingMarkupBuilder xml = new StreamingMarkupBuilder()
  xml.encoding = "UTF-8"

  def doc = xml.bind {
    mkp.xmlDeclaration()
    html {
      head () {
      }
      body{

	table {
	  tr {
	    th ("Reading")
	    th ("Image")
	  }
	  reader.readAll().each { ln ->
	    if (ln.size() >= 3) {
	      tr {
		CtsUrn urn
		String urnVal
		try {
		  urn = new CtsUrn(ln[1])
		  urnVal = urn.getSubref1()
		  td (urnVal)
		  td {
		    img(src : "${urnBase}${ln[2]}")
		  }

		} catch (Exception e) {
		  System.err.println "Bad CTS URN: " + ln[1]
		}
	      }
	    }
	  }
	}
      }
    }
  }
  outputFile.setText(doc.toString(), "UTF-8")
  System.err.println "Paleography observations in ${outputDir}/viewer.html"
}
/*end palView task */


task setUpVisualInventory(type: Copy) {
  description="Copy template files for visual inventory to build area."
  from "visualinventory"
  into "${buildDir}/visualinventory"
}


task indexScholia() {
  description = "Reads inventory of scholia, and creates separate indices for surfaces and images"
  //outputs.file "${buildDir}/scholiaToTbs.csv"
}


indexScholia.doLast {
  
  if (!buildDir.exists()) {
    buildDir.mkdir()
  }
  File scholiaToTbs = new File(buildDir, "scholiaToTbs.csv")
  File scholiaToImg = new File(buildDir,"scholiaToImage.csv")
  //println "WORK FROM scholia inventories "  + scholiaInventories

  scholiaInventories.split(/,/).each { inv ->
    //String inv = it.replace(/#REPO#/,baserepo)
    // println "repo prop ${baserepo} yields SCHOLIA INV " + inv
    if (inv) {
      File invFile = new File(inv)
      //System.err.println "Copy contents of scholia index file " + inv
      if (invFile) {
	CSVReader reader = new CSVReader(new FileReader(invFile))
	//System.err.println "Contents are " + invFile.getText("UTF-8")
	reader.readAll().each { ln ->
	  // Format: scholion, image, folio
	  if (ln.size() >= 3) {
	    scholiaToTbs.append(ln[0] + ',"' + ln[2] + '"\n')
	    scholiaToImg.append(ln[0] + ',"' + ln[1] + '"\n')
	  }
	}
      }
    }
  }
}


/* DSE analysis for a physical surface, or a range
 * of physical surfaces, identified by URN value
 * of the project property "folio".
 */
task dse(dependsOn : [setUpVisualInventory, indexScholia]) {
}
dse.doLast {

  Integer debugDse = 0
  
  CiteUrn urn
  try {
    urn  = new CiteUrn(folio)
  } catch (Exception e) {
    System.err.println "Unable to make urn for ${folio}"
    throw e
  }

  System.err.println "DSE analysis for URN " + urn
  
  def scorecard = []
  def reptDetails = [:]
  Integer totalGood = 0
  

  DseManager dse = new DseManager()

  if (debugDse > 0) {
    println "DSE configuration:"
    println "FolioToImage: " + folioToImage
    println "TextToImage: " + textToImage

  }

  folioToImage.split(/,/).each { idx ->
    //String idx = it.replace(/#REPO#/,baserepo)
    if (dse.tbsImageIndexFiles) {
      dse.tbsImageIndexFiles.add(new File(idx))
    } else {
      dse.tbsImageIndexFiles = [new File(idx)]
    }
  }

  
  textToImage.split(/,/).each { idx ->
    //String idx = it.replace(/#REPO#/,baserepo)
    if (dse.textImageIndexFiles) {
      dse.textImageIndexFiles.add(new File(idx))
    } else {
      dse.textImageIndexFiles = [new File(idx)]
    }
  }
  dse.textImageIndexFiles.add(new File (buildDir, "scholiaToImage.csv"))

  
  textToFolio.split(/,/).each { idx ->
    // String idx = it.replace(/#REPO#/,baserepo)
    if (dse.textTbsIndexFiles) {
      dse.textTbsIndexFiles.add(new File(idx))
    } else {
      dse.textTbsIndexFiles = [new File(idx)]
    }
  }
  dse.textTbsIndexFiles.add(new File (buildDir, "scholiaToTbs.csv"))

  if (debugDse > 0) {println "Created DSE Manager"}




  def range = urn.getObjectId().split(/-/)
  if (range.size() == 2) {
    System.err.println "Pages ranges are not currently suppored.  Please validate one page at a time."

    
    /*
    String startUrn = "urn:cite:${urn.getNs()}:${urn.getCollection()}.${range[0]}"
    String endUrn = "urn:cite:${urn.getNs()}:${urn.getCollection()}.${range[1]}"
    File venAseq = new File(codexModel)
    println "Running DSE from folio " +startUrn + " to " + endUrn
    boolean inSeq = false

    Integer lineCount = 0
    venAseq.eachLine { l ->
      if (lineCount > 0) {
	def cols = l.split(/,/)
	String currUrnStr = cols[1]
	CiteUrn currUrn
	try {
	  currUrn = new CiteUrn(currUrnStr)
	} catch (Exception e) {
	  throw new Exception("Could not make URN from ${currUrnStr}")
	}

	if (cols[1] == startUrn) {
	  inSeq = true
	}

	if (inSeq)  {
	  def rept = dse.dseReport(currUrn)
	  reptDetails[currUrn.toString()] = rept

	  String xml =  dse.getVisualInventoryXml(currUrn)
	  String fName = "${buildDir}/visualinventory/${urn.getCollection()}_${currUrn.getObjectId()}.xml"
	  File visinv = new File(fName)
	  visinv.setText(xml,"UTF-8")
	  boolean passfail = dse.verifyTbs(currUrn)
	  if (passfail) { totalGood++ }
	  def score = [currUrn,passfail]
	  scorecard.add(score)
	}
	
	if (cols[1] == endUrn) {
	  inSeq = false
	}
      }
      lineCount++;
    }

    */


  } else {

    System.err.println "Found ${dse.textNodesForSurface(urn).size()} text nodes, breaking down as follows: "
    dse.textTbsIndexFiles.each { f ->
      System.err.println "\tin the index ${f}, " + dse.textNodesForSurface(urn, f).size() + " nodes"
    }
    System.err.println "Using default image " + dse.imageForTbs(urn)  + ", found ${dse.textNodesForImage(dse.imageForTbs(urn)).size()} text nodes"
    
    


    def rept = dse.dseReport(urn)

    reptDetails[urn.toString()] = rept

    String xml =  dse.getVisualInventoryXml(urn)
    String fName = "${buildDir}/visualinventory/${urn.getCollection()}_${urn.getObjectId()}.xml"
    File visinv = new File(fName)
    visinv.setText(xml,"UTF-8")
    boolean passfail = dse.verifyTbs(urn)
    if (passfail) { totalGood++ }
    def score = [urn,passfail]
    scorecard.add(score)

  }


  def xml = new groovy.xml.StreamingMarkupBuilder().bind {
    
    html {
	head {
	  title("DSE validation for ${urn.getCollection()}, pages ${urn.getObjectId()}")
	}
	body {
	  h1("DSE validation for ${urn.getCollection()}, pages ${urn.getObjectId()}")
	  h2("Summary")
	  p {
	    mkp.yield "Totals:  "
	    strong "${scorecard.size()}"
	    mkp.yield " pages examined, "
	    strong "${totalGood}"
	    mkp.yield " pages pass DSE validation."
	  }
	  h2("Page-by-page record")
	  p {
	    mkp.yield "Visual inventories for individual pages are in the "
	    code("visualinventory")
	    mkp.yield "subdirectory."
	  }

	  
	  table {

	    tr {
	      th("Page")
	      th("Summary")
	      th("See visual inventory")
	      th("Default image")
	      th("Same texts indexed to image and surface")
	      th("Notes")
	      
	    }
	    scorecard.each { pair ->
	      CiteUrn pg = pair[0]
	      String defaultImg = "none found"
	      
	      String bgcolor = "#ffb0b0"
	      String summary = "Failed one or more tests."
	      if (pair[1]) {
		bgcolor = "#afa"
		summary = "Passed all tests."
	      }
	      tr {
		td("${pg}")
		td(style: "padding: 0.2em; background-color: ${bgcolor};",summary)
		td {

		  a (href: "visualinventory/${pg.getCollection()}_${pg.getObjectId()}.xml", "inventory for ${pg.getObjectId()}")
		}

		def details = reptDetails[pg.toString()]
		def imgRept = details[0]
		def mappingRept = details[1]

		bgcolor = "#ffb0b0"

		if (imgRept[0]) {
		  bgcolor = "#afa"
		  defaultImg = imgRept[1]
		}
		td(style: "padding: 0.2em; background-color: ${bgcolor};",defaultImg)

		bgcolor = "#ffb0b0"
		if (mappingRept[0]) {
		  bgcolor = "#afa"
		}
		td(style: "padding: 0.2em; background-color: ${bgcolor};",mappingRept[1])

		
		def txtCheck = dse.textNodesForSurface(pg)
		if (txtCheck.size() == 0) {
		  td(style: "padding: 0.2em; background-color: #ffffcc;", "Caution: no text units found for this page.")

		  
		} else {
		  // check for no Iliad text
		  String iliad = "urn:cts:greekLit:tlg0012.tlg001.msA:"
		  def iMapKeys = dse.imageMapsByText(defaultImg).keySet()
		  if (! iMapKeys.contains(iliad)) {
		    td(style: "padding: 0.2em; background-color: #ffffcc;") {
		      strong("Caution")
		      mkp.yield ": no "
		      em("Iliad")
		      mkp.yield (" text found for this page.")
		    }
		    
		  } else {
		    td("")
		  }
		}
	      }
	    }
	  }
	}
    }
  }

  
  File totals = new File("${buildDir}/${urn.getObjectId()}.html")
  totals.setText(xml.toString(), "UTF-8")
  
  println ""
  println "Results of DSE validation are in ${buildDir}/${urn.getObjectId()}.html"
}


task tabulate(dependsOn: dse) {
  description = "Writes tabulated form of all inventoried texts in ${buildDir}/tabulated."
}


tabulate.doLast {
  File tiFile = new File(textInventory)
  File archiveDir = new File(textArchive)
  File schemaFile = new File(invSchema)
  
  File tabDir = new File(buildDir,"tabulated")
  if (! buildDir.exists()) { 
    buildDir.mkdir()
  }
  if (! tabDir.exists()) {
    tabDir.mkdir()
  }

  Corpus c = new Corpus(tiFile, archiveDir, schemaFile)
  c.debug = 5
  c.tabulateRepository(tabDir)
}

task textsForSurface(dependsOn: tabulate) {
  description="Selects tabulated text data for requested text-bearing surface."
}

textsForSurface.doLast {
  // input and output files:
  File tabsDir = new File(buildDir, "tabulated")

  // Get a hocuspocus corpus for text repository:
  File archiveDir = new File(textArchive)
  File tiFile = new File(textInventory)
  File schemaFile = new File(invSchema)
  Corpus corpus = new Corpus(tiFile, archiveDir, schemaFile)

  // configure a DseManager:
  DseManager dse = new DseManager()

  folioToImage.split(/,/).each { idx ->
    if (dse.tbsImageIndexFiles) {
      dse.tbsImageIndexFiles.add(new File(idx))
    } else {
      dse.tbsImageIndexFiles = [new File(idx)]
    }
  }

  textToImage.split(/,/).each { idx ->
    if (dse.textImageIndexFiles) {
      dse.textImageIndexFiles.add(new File(idx))
    } else {
      dse.textImageIndexFiles = [new File(idx)]
    }
  }
  dse.textImageIndexFiles.add(new File(buildDir, "scholiaToImage.csv"))

  textToFolio.split(/,/).each { idx ->
    if (dse.textTbsIndexFiles) {
      dse.textTbsIndexFiles.add(new File(idx))
    } else {
      dse.textTbsIndexFiles = [new File(idx)]
    }
  }
  dse.textTbsIndexFiles.add(new File (buildDir, "scholiaToTbs.csv"))

  //dse.debug = 5

  // get relevant tabular data, and tokenize it:
  ArrayList tabData = dse.tabDataForSurface(folio, corpus, tabsDir)

  if (dse.debug > 5) {
    println "dse.tabDataForSurface yields " + tabData.size() + " entries, from "
    println "tabsDir: " + tabsDir
  }

  File outputDir = new File(buildDir, "surfaceTabs")
  if (! outputDir.exists()) {
    outputDir.mkdir()
  }

  File tabDataFile = new File(outputDir, "tab.txt")
  tabData.each {
    tabDataFile.append(it + "\n", "UTF-8")
  }
  
  if (hasProperty('debug')) {
    println "debug is on."
  }
    println "textsForSurface settings:"
    println "\tcorpus built from " + textArchive + " with inventory " + textInventory
    println "\ttabsDir was " + tabsDir 
    //}

}


  //task tokenize {
task tokenize(dependsOn: textsForSurface) {
  description = "Creates HMT classified tokenization of texts for given page"
  
}

tokenize.doLast {
  // input and output files:
  File surfTabsDir = new File(buildDir, "surfaceTabs")
  File outputDir = new File(buildDir, "tokens")
  if (! outputDir.exists()) {
    outputDir.mkdir()
  }
  File tokensFile = new File(outputDir, "tokens.csv")

  String fldSeparator = "#"
  //HmtTokenizer hmtTokens = new HmtTokenizer(surfTabsDir, tokensFile, fldSeparator)
  //hmtTokens.tokenizeTabs()

  File tabSrc = new File(surfTabsDir, "tab.txt")
  HmtEditorialTokenization toker = new HmtEditorialTokenization()
  def tokenizationResults = toker.tokenizeTabFile(tabSrc,"#")

  tokenizationResults.each { res ->
    tokensFile.append("${res[0]},${res[1]}\n")
  }
  println "Valid tokens: " + tokenizationResults.size()    
  println "Tokenization data in " +  tokensFile
}

task initVerification(type: Copy) {
  from "css"
  into "${buildDir}/verification/css"
}

task persNames(dependsOn: [tokenize, initVerification]) {
  description  = "Check that personal name identifiers are valid."
}
persNames.doLast {
  File summaries = new File(buildDir,"summaries.txt")
  File verificationDir = new File(buildDir, "verification")

  File tokens = new File("${buildDir}/tokens/tokens.csv")
  if (! tokens.exists()) {
    println "HEY!  No tokens file ${tokens}!!!"

  } else {
    def persNames = tokens.readLines().findAll { l -> l ==~ /.+,urn:cite:hmt:pers.+/}
    def occurrenceMap = [:]
    persNames.each { p ->
      def cols = p.split(/,/)
      println "Personame column : " + cols
      def pname = cols[1]
      def psg = cols[0]
      if (occurrenceMap[pname]) {
	def psgs = occurrenceMap[pname]
	psgs.add(psg)
	occurrenceMap[pname] =  psgs
      } else {
	occurrenceMap[pname] = [psg]
      }
    }

    def personsList = []
    File personsFile = new File (persnames)
    personsFile.eachLine {
      def cols = it.split(/,/)
      personsList.add(cols[0])
    }
    println "Check " + persNames.size() + " personal name occurrences  against " + personsList.size() + " authority list entries."
    
    Integer good = 0
    Integer bad = 0
    def scoreMap = [:]
    occurrenceMap.keySet().each { k ->
      if (personsList.contains(k)) {
	good++
	  
      } else {
	bad++
      }
    }
    summaries.append("Personal names,${good},${bad},${occurrenceMap.keySet().size()},personalnames.html\n")
    File persReport = new File(verificationDir, "personalnames.html")

    def personsXml = new groovy.xml.StreamingMarkupBuilder().bind {
      //mkp.pi("xml-stylesheet": "type='text/xsl' href='xslt/image.xsl'" )
      html {
	head {
	  title ("Verification of personal names for ${folio}")
	  link(type: "text/css", rel: "stylesheet", href: "css/hmt-core.css", title: "HMT CSS")
	
	}
	body {
	  header(role: "banner") {
	    mkp.yield "HMT-MOM: "
	    nav(role: "navigation") {
	      ul {
		li {
		  a(href:"index.html", "summary")
		}
	      }
	    }
	  }
	  article(role: "main") {
	    h1("Personal names for ${folio}")
	    table {
	      tr {
		th ("Reference")
		th ("Valid?")
		th ("Occurs in")
	      }
	      occurrenceMap.keySet().each { k ->
		tr {
		  td ("${k}")
		  td {
		    if (personsList.contains(k)) {
		      img(src: "css/check.png")
		    } else {
		      img(src: "css/delete.png")
		    }
		  }
		  td {
		    def psgs = occurrenceMap[k]
		    if (psgs.size() == 1) {
		      mkp.yield "${psgs[0]}"
		    } else {
		      ul {
			psgs.each { p ->
			  li ("${p}")
			}
		      }
		    }
		  }
		}
	      }
	    }
	  }
	}
      }
    }
    persReport.setText(personsXml.toString())
    

    //  } else {
    //println "No tokens file."
  }
}


task ethnics(dependsOn: [tokenize, initVerification]) {
  description  = "Check that ethnic adjective identifiers are valid."
}
ethnics.doLast {
  File summaries = new File(buildDir,"summaries.txt")
  File verificationDir = new File(buildDir, "verification")

  File tokens = new File("${buildDir}/tokens/tokens.csv")
  if (tokens.exists()) {
    def ethnicNames = tokens.readLines().findAll { l -> l ==~ /.+,urn:cite:hmt:peoples.+/}
    def occurrenceMap = [:]
    ethnicNames.each { 
      def cols = it.split(/,/)
      def ename = cols[1]
      def psg = cols[0]
      if (occurrenceMap[ename]) {
	def psgs = occurrenceMap[ename]
	psgs.add(psg)
	occurrenceMap[ename] =  psgs
      } else {
	occurrenceMap[ename] = [psg]
      }
    }
    println "Using placenames list in " + placenames
    def ethnicsList = []
    File ethnicsFile = new File(placenames)
    ethnicsFile.eachLine {
      def cols = it.split(/,/)
      ethnicsList.add(cols[0])
    }
    println "Check " + ethnicNames.size() + " ethnic name occurrences  against " + ethnicsList.size() + " authority list entries."



    Integer good = 0
    Integer bad = 0
    def scoreMap = [:]

    occurrenceMap.keySet().each { k ->
      String place = k.replace("hmt:peoples","hmt:place")
      if (ethnicsList.contains(place)) {
	good++

      } else {
	bad++
      }
    }
    println "${good}/${bad}/${occurrenceMap.keySet().size()}"


    summaries.append("Ethnic adjectives,${good},${bad},${occurrenceMap.keySet().size()},ethnics.html\n")
    File ethnicReport = new File(verificationDir, "ethnics.html")

    def ethnicsXml = new groovy.xml.StreamingMarkupBuilder().bind {
      //mkp.pi("xml-stylesheet": "type='text/xsl' href='xslt/image.xsl'" )
      html {
	head {
	  title ("Verification of ethnic adjectives for ${folio}")
	  link(type: "text/css", rel: "stylesheet", href: "css/hmt-core.css", title: "HMT CSS")
	
	}
	body {
	  header(role: "banner") {
	    mkp.yield "HMT-MOM: "
	    nav(role: "navigation") {
	      ul {
		li {
		  a(href:"index.html", "summary")
		}
	      }
	    }
	  }
	  article(role: "main") {
	    h1("Ethnic adjectives for ${folio}")
	    table {
	      tr {
		th ("Reference")
		th ("Valid?")
		th ("Occurs in")
	      }
	      occurrenceMap.keySet().each { k ->
		String place = k.replace("hmt:peoples","hmt:place")
		tr {
		  td ("${k}")
		  td {
		    if (ethnicsList.contains(place)) {
		      img(src: "css/check.png")
		    } else {
		      img(src: "css/delete.png")
		    }
		  }
		  td {
		    def psgs = occurrenceMap[k]
		    if (psgs.size() == 1) {
		      mkp.yield "${psgs[0]}"
		    } else {
		      ul {
			psgs.each { p ->
			  li ("${p}")
			}
		      }
		    }
		  }
		}
	      }
	    }
	  }
	}
      }
    }
    ethnicReport.setText(ethnicsXml.toString())


  } else {
    println "No tokens file."
  }
}




task placeNames(dependsOn: [tokenize, initVerification]) {
  description  = "Check that place name identifiers are valid."
}

placeNames.doLast {

  File summaries = new File(buildDir,"summaries.txt")
  File verificationDir = new File(buildDir, "verification")
  File tokens = new File("${buildDir}/tokens/tokens.csv")
  if (tokens.exists()) {
    def placeNames = tokens.readLines().findAll { l -> l ==~ /.+,urn:cite:hmt:place.+/}
    def placeList = []
    File placeFile = new File (placenames)
    placeFile.eachLine {
      def cols = it.split(/,/)
      placeList.add(cols[0])
    }
    println "Check " + placeNames.size() + " place name occurrences  against " + placeList.size() + " authority list entries."


    def placeOccurrenceMap = [:]
    placeNames.each { 
      def cols = it.split(/,/)
      def pname = cols[1]
      def psg = cols[0]
      if (placeOccurrenceMap[pname]) {
	def psgs = placeOccurrenceMap[pname]
	psgs.add(psg)
	placeOccurrenceMap[pname] =  psgs
      } else {
	placeOccurrenceMap[pname] = [psg]
      }
    }




    Integer goodPlace = 0
    Integer badPlace = 0
    placeOccurrenceMap.keySet().each { k ->
      if (placeList.contains(k)) {
	goodPlace++
      } else {
	badPlace++
      }
    }

    summaries.append("Place names,${goodPlace},${badPlace},${placeOccurrenceMap.keySet().size()},placenames.html\n")
    File placeReport = new File(verificationDir, "placenames.html")

    def placesXml = new groovy.xml.StreamingMarkupBuilder().bind {
      //mkp.pi("xml-stylesheet": "type='text/xsl' href='xslt/image.xsl'" )
      html {
	head {
	  title ("Verification of place names for ${folio}")
	  link(type: "text/css", rel: "stylesheet", href: "css/hmt-core.css", title: "HMT CSS")
	
	}
	body {
	  header(role: "banner") {
	    mkp.yield "HMT-MOM: "
	    nav(role: "navigation") {
	      ul {
		li {
		  a(href:"index.html", "summary")
		}
	      }
	    }
	  }
	  article (role: "main") {
	    h1("Place names for ${folio}")
	    table {
	      tr {
		th ("Reference")
		th ("Valid?")
		th ("Occurs in")
	      }
	      placeOccurrenceMap.keySet().each { k ->
		tr {
		  td ("${k}")
		  td {
		    if (placeList.contains(k)) {
		      img(src: "css/check.png")
		    } else {
		      img(src: "css/delete.png")
		    }
		  }
		  td {
		    def psgs = placeOccurrenceMap[k]
		    if (psgs.size() == 1) {
		      mkp.yield "${psgs[0]}"
		    } else {
		      ul {
			psgs.each { p ->
			  li ("${p}")
			}
		      }
		    }
		  }
		}
	      }
	    }
	  }
	}
      }
    }
    placeReport.setText(placesXml.toString())

  } else {
    println "No tokens file."
  }
}

task initReport {
  description = "Initializes a new verification report."
  if (! buildDir.exists()) {
    buildDir.mkdir()
  }
  File summaries = new File(buildDir,"summaries.txt")
  summaries.setText("label,success,failure,total,link\n")
  File verificationDir = new File(buildDir, "verification")
  if (! verificationDir.exists()) {
    verificationDir.mkdir()
  }
}

task copyCss(type: Copy) {
  from "css"
  into "${buildDir}/verification/css"
}


//task lex(dependsOn: tokenize) {
//task lex(dependsOn: [tokenize, initVerification]) {
task lex() {
  description = "Analyzes lexical tokens."
}

lex.doLast {
  // All tokens, as created by tokenize task:
  File tokens = new File("${buildDir}/tokens/tokens.csv")

  if (tokens.exists()) {
    // 2 data structures:
    // 1. Map of unique tokens to passages:
    def passageMap = [:]

    // 2. record of analysis:  analyze each token as one of 
    // a) byz (appears in byz ortho list), 
    // b) success (morpheus parses), 
    // c) fail (neither of the preceding)
    def statusMap = [:]

    // Lexical tokens for morpheus to analyze:
    File lexTokensFile = new File("${buildDir}/tokens/lextokens.txt")


    def lexItems = tokens.readLines().findAll { l -> l ==~ /.+,urn:cite:hmt:tokentypes.lexical/}
    println "Unique lexical items: " + lexItems.unique().size() + " (${lexItems.size()} total)"

    lexItems.each { 
      def cols = it.split(/,/)
      def lex = cols[1]
      def psg = cols[0]
      System.err.println "Lex cols " + cols 
      if (passageMap[lex]) {
	def psgs = passageMap[lex]
	psgs.add(psg)
	passageMap[lex] =  psgs
      } else {
	passageMap[lex] = [psg]
      }
      
    }

    TransCoder tobeta = new TransCoder()
    tobeta.setConverter("BetaCode")
    tobeta.setParser("Unicode")

    File summaries = new File(buildDir,"summaries.txt")
    File verificationDir = new File(buildDir, "verification")

    println "Use byz ortho in " + byzortho
    File byzorth = new File( byzortho)

    def orthoAuthorityList = []
    Integer count = 0
    byzorth.eachLine { l ->
      if (count > 0) {
	def cols = l.split(/,/)
	if (l.size() == 0) {
	  // omit
	} else {
	  if (cols[3] != "rejected") {
	    orthoAuthorityList.add( tobeta.getString(cols[1]) )
	  }
	}
      }
      count = count + 1
    }

    Integer success = 0
    Integer fail = 0
    Integer lexCount = 0

    lexItems.each { lex ->
      lexCount = lexCount + 1
      def cols = lex.split(/,/)
      CtsUrn tokenUrn = new CtsUrn(cols[0])
      String token = tokenUrn.getSubref()
      String betaToken = tobeta.getString(token.toLowerCase())
      
      if ((token.size() < 1) || (token == "⁑")){
	println "${lexCount}: omit punctuation ${token}"
      } else if (orthoAuthorityList.contains(betaToken)) {

	println "${lexCount}: Byzantine orthography ok: " + token
	statusMap[tokenUrn] = "byz"

      } else {

	def command = "${morpheus} ${betaToken}"
	print "${lexCount}: Analyzing ${token} with ${command}..."
	def proc = command.execute()
	proc.waitFor()
	def reply = proc.in.text.readLines()

	if (reply[1] ==~ /.*unknown.+/) {
	  statusMap[tokenUrn]  = "fail"
	  fail = fail + 1
	  println " fail."
	} else {
	  statusMap[tokenUrn]  = "success"
	  success = success + 1
	  println " success."
	}
      }
    }


    summaries.append("Lexical tokens,${success},${fail},${success + fail},lextokens.html\n")  

    File lexReport = new File(verificationDir, "lextokens.html")
    def lexXml = new groovy.xml.StreamingMarkupBuilder().bind {
      //mkp.pi("xml-stylesheet": "type='text/xsl' href='xslt/image.xsl'" )
      html {
	head {
	  title ("Verification of lexical tokens for ${folio}")
	  link(type: "text/css", rel: "stylesheet", href: "css/hmt-core.css", title: "HMT CSS")
	
	}
	body {
	  header(role: "banner") {
	    mkp.yield "HMT-MOM: "
	    nav(role: "navigation") {
	      ul {
		li {
		  a(href:"index.html", "summary")
		}
	      }
	    }
	  }
	  article (role: "main") {
	    h1("Lexical tokens for ${folio}")
	    table {
	      tr {
		th ("Reference")
		th ("Valid?")
		//		th ("Occurs in")
	      }

	      // cycle each token in status map
	      // report its passages from occurrence map
	      statusMap.keySet().each { k ->
		tr {
		  td(k)
		  td {
		    if (statusMap[k] == "byz") {
		      img(src: "css/check.png")
		      
		    } else if (statusMap[k] == "success") {
		      img(src: "css/check.png")
		    } else {
		      img(src: "css/delete.png")
		    }
		  }
		  /*
		  td {
		    def psgs =  passageMap[k]
		    
		    if (psgs != null) {
		      if (psgs.size() > 1) {
			ul {
			  psgs.each {
			    li(it)
			  }
			}
		      } else {
			mkp.yield (psgs[0])
		      }
		    }
		  } */
		}
	      }
	    }
	  }
	}
      }
    }
    lexReport.setText(lexXml.toString())

  } else {
    println "No tokens file."
  }

}



task cpCheck(dependsOn: tabulate) {
  description = "Checks that unicode code point usage is valid for given document."
}
cpCheck.doLast {
  //println "Validation of unicode code point usage not yet implemented."
}


task verify(dependsOn: [initReport, persNames, placeNames, lex, ethnics, copyCss, cpCheck]) {
    description = "Runs full verification suite on requested URN(s)"
}
verify.doFirst {
  System.out.println "Preparing report on folio ${folio}\n"
}
verify.doLast {
  DseManager dse = new DseManager()
  dse.debug = 0
  
  File summaries = new File(buildDir, "summaries.txt")

  def reportXml = new groovy.xml.StreamingMarkupBuilder().bind {
    //mkp.pi("xml-stylesheet": "type='text/xsl' href='xslt/image.xsl'" )
    html {
      head {
	title ("Summary of verification for ${folio}")
	link(type: "text/css", rel: "stylesheet", href: "css/hmt-core.css", title: "HMT CSS")
      }
      body {
	header(role: "banner") {
	  mkp.yield "HMT-MOM: "
	  nav(role: "navigation") {
	    ul {
	      li {
		a(href:"index.html", "summary")
	      }
	    }
	  }
	}
	article (role: "main") {
	  h1 ("Page ${folio} — summary of verification")
	  p {
	    strong("MOM version: ${version}")
	  }
	  p("DSE reports are not yet integrated in this web page:  see separate feedback in terminal.")
	  p {
	    mkp.yield "Also see a "
	    a(href: "../visualinventory/", "visual inventory")
	    mkp.yield " of indexed text passages. "
	    mkp.yield "(Open with Safari or Firefox.)"
	  }


	  table {
	    tr {
	      th("Report")
	      th("Success/Failure/Total")
	      th("Details")
	    }
	    Integer reportCount = 0
	    summaries.eachLine { ln ->
	      reportCount++
		if (reportCount > 1) {
		  def cols = ln.split(/,/)
		  Integer success = cols[1].toInteger()
		  Integer fail = cols[2].toInteger()
		  Integer total = cols[3].toInteger()
		  tr {
		    td {
		      if  (success == total) {
			img(src: "css/check.png")
			mkp.yield(cols[0])
		      } else {
			img(src: "css/delete.png")
			mkp.yield(cols[0])
		      }

		    }
		    td("${cols[1]}/${cols[2]}/${cols[3]}")
		    td {
		      a(href : "${cols[4]}", "details")
		    }
		  }
		}
	    }
	  }
	}
      }
    }
  }

  File summaryReport = new File("${buildDir}/verification/index.html")
  summaryReport.setText(reportXml.toString())

  System.out.println "Completed verification of ${folio}."
  System.out.println "See a summary of results in ${buildDir}/verification/index.html."

}


// for specs with concordion:
sourceSets {
    main {
        java {
            srcDir 'src'
        }
        resources {
            srcDir 'src'
        }
    }
    test {
        java {
	  srcDir "specs/java"
        }
        resources {
	  srcDir "${buildDir}/specs"
        }
    }
}

task cpResources(type: Copy) {
  from "specs/resources"
  into "${buildDir}/specs"
}

task setUpResources(dependsOn: cpResources) {
}
setUpResources.doLast {
  println "READING FILE TREE FROM " + mdSrc
  FileTree tree = fileTree(mdSrc) {
    include "**/*.md"
  }
  tree.visit { f ->
    if (f.relativePath.isFile()) {
      File inFile = new File("${mdSrc}/${f.relativePath}")
      println "Need to work on " + inFile
      def segs = f.relativePath.getSegments()
      String treePath = "${buildDir}/specs"
      Integer limit =  segs.size() - 1
      segs.eachWithIndex { s, i ->
	if (i < limit) {
	  treePath = "${treePath}/${s}"
	  File nxtDir = new File(treePath)
	  if (! nxtDir.exists()) {
	    nxtDir.mkdir()
	  }
	}
      }
      File outDir = new File(treePath)
      String htmlFileName = f.relativePath.getLastName().replaceFirst(/.md$/,".html")
      File htmlFile = new File(outDir, htmlFileName)
      println "Created ${htmlFile}"

      String body = Processor.process(inFile.getText("UTF-8"),Configuration.DEFAULT)
      htmlFile.setText("${htmlPreface}${body}${htmlEnd}", "UTF-8")
    }
  }
}


test.dependsOn setUpResources
test {
    systemProperties 'concordion.output.dir': file("${buildDir}/concordion-results")

}

task addVersion(){
}
addVersion.doLast {
  tokenMap["version"] = version
  tokenMap["deps"] = """
    <tr><td><a href="http://cite-architecture.github.io/cite/">cite</a></td><td>${citeVersion}</td></tr>
  <tr><td><a href="http://cite-architecture.github.io/citemgr/">citemgr</a></td><td>${citemgrVersion}</td></tr>

  <tr><td><a href="http://homermultitext.github.io/hmt-utils/">hmt-utils</a></td><td>${hmtutilsVersion}</td></tr>
  
  <tr><td><a href="http://cite-architecture.github.io/hocuspocus/">hocuspocus</a></td><td>${hocuspocusVersion}</td></tr>
  <tr><td>greekutils</td><td>${greekutilsVersion}</td></tr>
  <tr><td>Epidoc transcoder</td><td>${transcoderVersion}</td></tr>
""".toString() 
}

task conc(type: Copy, dependsOn: [test, addVersion]) {
//task conc(type: Copy) {
  from "${buildDir}/concordion-results"
  into "${buildDir}/concordion-formatted"
  filter(ReplaceTokens, tokens: tokenMap)
}
